{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e4547f9",
   "metadata": {},
   "source": [
    "# Exercise: Implement the ReLU Activation Function\n",
    "\n",
    "In this exercise, you'll implement one of the most widely used activation functions in deep learning — **ReLU**, or **Rectified Linear Unit**.\n",
    "\n",
    "ReLU introduces **non-linearity** into the neural network while being simple and computationally efficient.\n",
    "\n",
    "Let’s dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e6704",
   "metadata": {},
   "source": [
    "## What Is ReLU?\n",
    "\n",
    "The ReLU activation function is defined as:\n",
    "\n",
    "$ A = ReLU(x) = max(0, x)$\n",
    "\n",
    "This means:\n",
    "- If the input `x` is **positive**, it returns `x`.\n",
    "- If the input `x` is **negative or zero**, it returns `0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de57bd0",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914f8c7",
   "metadata": {},
   "source": [
    "## Task: Create ReLU function\n",
    "\n",
    "Complete the function `relu(x)` to return the ReLU of `x`.\n",
    "\n",
    "Your function should:\n",
    "- Accept either a **scalar** or a **NumPy array**\n",
    "- Replace all negative values with 0.0 (yes, specifically 0.0 and not 0. This will is important for later as we train a simple model ensuring all scalar values match data types)\n",
    "- Return a scalar or array (same shape as input)\n",
    "\n",
    "Hint: Use TensorFlow's built-in `maximum()` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the ReLU (Rectified Linear Unit) of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    ReLU applied element-wise to x\n",
    "    \"\"\"\n",
    "    result = # Replace with your code\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fab2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_relu():\n",
    "    \"\"\"\n",
    "    Test the relu function with scalar inputs.\n",
    "    \"\"\"\n",
    "    # Test relu function with negative values\n",
    "    assert relu(-5) == 0, \"Scalar negative input test failed. Expected 0 for negative input.\"\n",
    "    # Test relu function with positive values\n",
    "    assert relu(3) == 3, \"Scalar positive input test failed. Expected x for positive input for relu(x).\"\n",
    "    # Test relu function with zero\n",
    "    assert relu(0) == 0, \"Scalar zero input test failed. Expected 0 for zero input.\"\n",
    "\n",
    "    print(\"Scalar tests passed!\")\n",
    "\n",
    "def test_relu_array():\n",
    "    \"\"\"\n",
    "    Test the relu function with numpy array inputs.\n",
    "    \"\"\"\n",
    "    input_array = np.array([-1, 0, 2])\n",
    "    expected_output = np.array([0, 0, 2])\n",
    "    try:\n",
    "        np.testing.assert_array_equal(relu(input_array), expected_output)\n",
    "        print(\"Array test passed!\")\n",
    "    except AssertionError:\n",
    "        raise AssertionError(\"Array test failed: expected [0, 0, 2] from relu([-1, 0, 2])\")\n",
    "    \n",
    "\n",
    "try:\n",
    "    test_relu()\n",
    "    test_relu_array()\n",
    "    print(\"All tests passed!\")\n",
    "except AssertionError as err:\n",
    "    print(err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38c3e9",
   "metadata": {},
   "source": [
    "## Task: Plot ReLU function on a Graph.\n",
    "\n",
    "Plot the ReLU function for values from -10 to 10.\n",
    "\n",
    "This helps you visualize how ReLU behaves depending on the value of x.\n",
    "\n",
    "Hint: use the code above that you wrote above to fill in the values for y!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relu():\n",
    "    \"\"\"\n",
    "    Plot the ReLU activation function.\n",
    "    \"\"\"\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    y = # Replace with your code\n",
    "    plt.plot(x, y)\n",
    "    plt.title(\"ReLU Activation Function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"ReLU(x)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d2309",
   "metadata": {},
   "source": [
    "If run correctly, you should see a plot where the relationship between x and y is linear except for when $x < 0$, in which $y = 0$ for all of $x < 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cbffe7",
   "metadata": {},
   "source": [
    "## ReLU in a Real Neural Network (Using TensorFlow)\n",
    "\n",
    "So far, you've implemented and visualized the ReLU activation function manually.\n",
    "\n",
    "Now let’s see how it's used in practice — inside a real neural network, trained on real data!\n",
    "\n",
    "To do this, we’ll use **TensorFlow**, one of the most popular open-source libraries for building and training machine learning models. \n",
    "\n",
    "Additionally, we will use a high-level API called **Keras**, which assists with defining and training neural networks.\n",
    "\n",
    "#### Why TensorFlow?\n",
    "\n",
    "- TensorFlow handles low-level math (like gradients and matrix ops) for us.\n",
    "- TensorFlow makes it easy to define layers, activation functions, loss functions, and optimizers.\n",
    "\n",
    "In the code below,  build a neural network and use our custom `relu()` function as the activation function in the hidden layer.\n",
    "\n",
    "#### Dataset: MNIST\n",
    "\n",
    "We’ll use the **MNIST dataset** for training our neural network. **MNIST** is a dataset that contains grayscale images of handwritten digits (0–9). Each image is 28×28 pixels and we will build a classifier that correctly predicts the digit in each image.\n",
    "\n",
    "Let’s load the data and build the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341bb39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the images to values between 0 and 1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Flatten the 28x28 images to 784-dimensional vectors\n",
    "X_train = X_train.reshape(-1, 28 * 28)\n",
    "X_test = X_test.reshape(-1, 28 * 28)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ce3f4f",
   "metadata": {},
   "source": [
    "## The Neural Network Structure\n",
    "\n",
    "We'll build a simple neural network with:\n",
    "- **Input layer**: 784 features (28×28 pixels)\n",
    "- **Hidden layer**: 128 neurons with ReLU activation\n",
    "- **Output layer**: 10 neurons (one per digit) with softmax activation\n",
    "\n",
    "## Task: Incorporate ReLU into Neural Network\n",
    "Set the Dense hidden layer's activation function to your custom made ReLU function.\n",
    "\n",
    "**Hint**: Do not use activation='relu' as this defaults to Kera's relu function. \n",
    "\n",
    "**Hint** If you get an error saying `TypeError: relu() missing 1 required positional argument: 'x'`, remove the `()` from the method call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5558cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation=, input_shape=(784,)),  # Replace with your code\n",
    "    Dense(10, activation='softmax') \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a804046",
   "metadata": {},
   "source": [
    "## What ReLU Really Does in a Neural Network\n",
    "\n",
    "When you build a neural network and use `activation=relu`, you're telling the model:\n",
    "\n",
    "\"After the weighted sum of inputs is calculated at each neuron, apply the ReLU function to that result.\"\n",
    "\n",
    "\n",
    "### Step-by-Step Computation\n",
    "\n",
    "For a dense (fully connected) layer:\n",
    "\n",
    "$Z = W \\cdot X + b$\n",
    "\n",
    "- `W`: weight matrix\n",
    "- `X`: input vector\n",
    "- `b`: bias vector\n",
    "- `Z`: raw output (pre-activation)\n",
    "\n",
    "Then ReLU is applied element-wise to `Z`:\n",
    "\n",
    "$A = \\text{ReLU}(Z) = \\max(0, Z)$\n",
    "\n",
    "So if the output of a neuron is **negative**, ReLU turns it into **zero**. If it's **positive**, it passes through unchanged.\n",
    "\n",
    "### Why use ReLU?\n",
    "\n",
    "ReLU is particularly useful because by zeroing out negative values, it also adds a form of **sparsity** — only some neurons activate for a given input. Additionally, ReLU is computationally **faster** (just a single `max()` call), compared to other activation functions which can include exponential values.\n",
    "\n",
    "### In Practice\n",
    "\n",
    "During training (see `.fit()` below), this ReLU logic is used during every forward pass:\n",
    "- Input → Linear transformation → ReLU → Next layer\n",
    "- ReLU is part of what's being \"trained around\" as weights are updated via backpropagation.\n",
    "\n",
    "Your `relu()` function is plugged directly into this pipeline — so every neuron in your hidden layer applies your implementation to its computed value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0913be66",
   "metadata": {},
   "source": [
    "## Verify Your ReLU Activation Is Used\n",
    "\n",
    "To ensure that your custom `relu` function was correctly passed into the model, you can inspect the model's layers.\n",
    "\n",
    "Run the following commands in the cell below to view information about the model's layers:\n",
    "\n",
    "```python\n",
    "model.layers[0].activation.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b591214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the activation function of the first layer\n",
    "model.layers[0].activation.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8dd78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c3203c",
   "metadata": {},
   "source": [
    "## Great job!\n",
    "\n",
    "You have successfully trained a neural network on MNIST data using the ReLU activation function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfb1fd",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "- ReLU activation helps neural networks learn non-linear functions efficiently.\n",
    "- It \"activates\" only positive values and zeroes out negatives.\n",
    "- It’s the default choice for hidden layers in modern deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831331e3",
   "metadata": {},
   "source": [
    "# Solution to exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b5882",
   "metadata": {},
   "source": [
    "## Solution: Create ReLU function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5111988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    result = tf.maximum(0.0, x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df87604f",
   "metadata": {},
   "source": [
    "## Solution: Plot ReLU function on a Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3da42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relu():\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    y = relu(x)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(\"ReLU Activation Function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"ReLU(x)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567521b8",
   "metadata": {},
   "source": [
    "## Solution: Incorporate ReLU into Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation=relu, input_shape=(784,)),\n",
    "    Dense(10, activation='softmax') \n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
